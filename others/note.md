# 一些笔记
  
## 时空和认知交互理论  
深度神经网络参数学习本质：线性变换嵌套非线性激活函数的多层组合拟合非线性函数  
### 基础网络结构   
#### 卷积神经网络 CNN  
处理空间数据，如图像  
本质：图像卷积相当于用卷积操作代替线性变换，减少参数量，提高计算效率  
#### 循环神经网络 RNN  
处理时间序列数据，如语音、文本  
本质：Y(t) = f(WX(t) + UY(t-1) + b)，Y(t)是当前时刻的输出，X(t)是当前时刻的输入，Y(t-1)是上一时刻的输出，W、U是权重矩阵，b是偏置，f是激活函数  
类似控制系统的状态空间方程，网络结构上就像引入了反馈机制  
发展：RNN -> 深度RNN -> 双向RNN -> 门控循环单元GRU -> 长短时记忆网络LSTM -> 编码器-解码器网络 -> 注意力机制  
#### 注意力神经网络 Transformer  
本质依旧是线性变换嵌套非线性激活函数的多层组合  
自注意力机制公式：  
$$Attention(Q, K, V) = softmax(\frac{XW^Q(XW^K)^T}{\sqrt{d_k}})XW^V$$  
交叉注意力机制公式：  
$$Attention(Q, K, V) = softmax(\frac{XW^Q(YW^K)^T}{\sqrt{d_k}})YW^V$$  
$$Y是隐藏层的输出，X是输入$$  
应用在视觉领域：
ViT：将图像分割成一系列patch，然后将patch序列化，输入到Transformer中  
#### 组合网络结构  
如深度生成模型：GAN、VAE  
编解码器思想：编码器将输入数据映射到潜在空间，解码器将潜在空间的数据映射到输出空间  
